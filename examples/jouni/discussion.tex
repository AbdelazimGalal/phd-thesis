\chapter{Conclusions}\label{chapter:conclusions}

\paragraph{Chapter~\ref{chapter:rlcsa}.}

We showed that compressed suffix arrays can achieve much better compression than predicted by empirical entropy, if the text is highly repetitive. This improved compression does not degrade the query performance of the index significantly. A partial exception is the \locate{} query, whose performance depends on the number of suffix array samples. While there has been some success in compressing the samples when a good multiple alignment of a collection of texts is known \cite{Maekinen2010,Huang2010}, they are still incompressible in the general case.

When analyzing the proposed index, we used the number of equal letter runs in the Burrows-Wheeler transform as our complexity metric, instead of the usual empirical entropy. We provided evidence that the number of runs scales with the complexity of the text, and hence is a good complexity metric, at least for highly repetitive texts. An interesting open question is, how does the number of runs relate to the number of phrases in a Lempel-Ziv parsing of the text. We get an expected case bound for the number of runs from Theorem~\ref{theorem:expected case runs} by treating each phrase as a constant number of edit operations. No worst-case bounds or bounds in the other direction are known, however.

\paragraph{Chapter~\ref{chapter:construction}.}

In many applications, the size of data sets increases with the amount of computing power available. One consequence of this is that when using compressed data structures, the space requirements of the construction algorithm often become the bottleneck that determines how large data sets we can process. We showed how to construct a compressed suffix array for a collection of length $N$ in $O(Nm)$ time and $O(N/m \log N)$ bits of extra working space by indexing the collection in $m$ parts. In many cases, this means that we can build the compressed suffix array in $O(n \log n)$ time while using less space than original size of the collection. As the algorithm parallelizes well, it can be used to index data sets of up to tens of gigabytes in size on current hardware.

One variant of the construction algorithm sorts the suffixes of a new subcollection by their lexicographic ranks among the suffixes of already indexed subcollections. This generalizes the well-known technique of packing several consecutive characters into a single machine word to speed up suffix sorting. While a straightforward application of the idea did not yield significant improvements in construction speed, there may be other ways to use the rank information to construct the suffix array faster.

\paragraph{Chapter~\ref{chapter:lcp}.}

We proposed an extremely space-efficient algorithm for constructing various compressed representations of the longest common prefix array directly from a compressed suffix array. While the algorithm is much slower than the alternatives with larger space requirements, it should parallelize well, making it competitive with space-efficient CSA construction. Yet if we have to construct the LCP array from scratch every time we insert new sequences into the collection, LCP construction will be the bottleneck in maintaining the index. An open question is, can we merge the LCP arrays of two text collections efficiently in a similar way as we merge the CSAs in the space-efficient construction algorithm.

We also showed how to derive any LCP value from a set of sampled values in a similar way as deriving suffix array values from samples in a \locate{} query. With regular (not highly repetitive) texts, this new sampled LCP array representation occupies the middle ground in time/space trade-offs, combining reasonable compression and relatively fast access to the LCP values. Yet if most of the minimal LCP values are small, it might be possible to achieve better compression without sacrificing too much performance by computing the small values directly. It remains to be seen, if some combination of sampling and direct LCP computation can provide improved time/space trade-offs.

Many compressed suffix tree proposals are based on combining a compressed suffix array, a compressed representation of the LCP array, and a representation of suffix tree topology. For the first two components, there exist solutions whose size depends on the number of equal letter runs in the Burrows-Wheeler transform, making them attractive for indexing highly repetitive collections. Unfortunately, no known solution for compressing the suffix tree topology combines such compression performance with fast queries. Finding such solution would make compressed suffix trees much more attractive for indexing highly repetitive collections.

\paragraph{Chapter~\ref{chapter:gcsa}.}

We generalized compressed suffix arrays for indexing finite automata. While the index can be exponentially larger than the automaton in the worst case, the size increase will remain small for highly repetitive languages such as those arising from a set of individual genomes. The generalized index is slower than a regular compressed suffix array by a factor of two, due to the use of an additional bit vector to map outgoing edges to nodes. Index construction still requires much more memory than with regular CSAs. It should be possible to adapt most of the algorithms using a CSA to use the generalized index instead.

A major open problem is, whether a similar generalization is possible for grammar-based indexes. Context-free grammars have many advantages over finite automata in describing finite languages. One of the advantages is the ability to express rearrangements: substrings that occur in different positions in different strings of the language.
