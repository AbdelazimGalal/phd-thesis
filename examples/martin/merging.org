% The dissertation proposal should:

% * MOTIVATION for pursuing the research
% ** building from source is too slow and memory intensive now
%%%% How do we motivate complex variant detection in growing populations? Do we know complex variants are important for survalence? 
% ** populations will only grow over time
% ** SNP based approaches don't capture all of the evolutionary changes, so loose sensitivity.

% Reference-free SNP detection: dealing with the data deluge -- discusses advantages of graphs
% https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-15-S4-S10

% SV/complex-variants in prokaryotes https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5316281/
% https://www.ncbi.nlm.nih.gov/pubmed/25189783

% http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0166162

% * precise STATEMENT OF THE PROBLEM to be solved,
% ** Efficiently build succinct colored de Bruijn graph
% ** Toward rapid complex variant based phylogenetic classification in growing sample populations
% ``complex'' - http://www.hgvs.org/mutnomen/PMID21309030_PTaschner_HM.pdf
%    -- referenced by http://varnomen.hgvs.org/ and http://www.hgvs.org/mutnomen/

% Real-Time Pathogen Detection in the Era of Whole-Genome Sequencing and Big Data: Comparison of k-mer and Site-Based Methods for Inferring the Genetic Distances among Tens of Thousands of Salmonella Samples
% http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0166162

% Prospective use of whole genome sequencing (WGS) detected a multi-country outbreak of Salmonella Enteritidis
%https://www.cambridge.org/core/journals/epidemiology-and-infection/article/prospective-use-of-whole-genome-sequencing-wgs-detected-a-multicountry-outbreak-of-salmonella-enteritidis/81731FED9068201D00D20E6902651EAF

% Characterization of Foodborne Outbreaks of Salmonella enterica Serovar Enteritidis with Whole-Genome Sequencing Single Nucleotide Polymorphism-Based Analysis for Surveillance and Outbreak Detection
%http://jcm.asm.org/content/53/10/3334.short

% * reflect an extensive critical LITERATURE SURVEY, and contain an accurate assessment of the state-of-the-art in the area of research,

% * evidence to the effect that there is a good likelihood the PROBLEM IS SOLVABLE with reasonable effort.
% ** preliminary results

The motivation for merging succinct colored de Bruijn graphs is that it can mitigate problems with the long time it can take to construct these data structures from scratch.  An example of this construction time is that it took the better part of a week to build the data structures for the 87 metagenomic samples in the VARI paper.  This could pose a problem if we want to apply this data structure to food safety screening projects such as GenomeTrakr.  In such projects, the goal is to isolate a pathogen from an infected patient presenting foodbourne illness and then track down the upstream source in the food supply chain.  The source is found by searching a data structure housing samples from wide swaths of food producers.  If the upstream source can be identified, any additionally tainted product in transit to consumers can be recalled and prevent further illness.  However, in order to identify the problem food source using the patient's isolated pathogen, a similar strain from the problem food source must already be in the database.  Thus, if the succinct de Bruijn graph must be rebuilt from scratch in order to include new samples, then there will be a lag before the newest samples would be findable, which are arguably the most important ones for saving people from infection.  This method would have applications in any project studying a population where more samples are acquired over time.

This document is organized as follows.  We begin by giving a background on biology and bioinformatics methods.  Then we present aspects of succinct data structures and report preliminary results of those aspects applied to bioinformatics problems.  Two important aspects we'll consider throughout are construction and navigation. 

Our contribution:
\begin{itemize}
\item A method for efficiently merging succinct colored de Bruijn graphs, having the following advantages:
  \begin{itemize}
  \item Reduced working space
  \item Allows incremental construction
    \item A new method for merging fixed prefix BWT.
  \end{itemize}
\item The first assesment of utility of complex variants for survelance purposes for population of this scale
  % it would be useful to show existance of complex variants which are necessary for distinguishing sources.  This 
  \end{itemize}
%%% research exam


The radix merge may be of benefit to other prefix-only compressed suffix arrays such as GCSA and XBW.
 

* Remaining Work

** Timeline

** Tasks

\begin{itemize}
\item How to iterate WT<RRR> (or alternately Alex's symbol-wise bitvectors)
\item How to iterate color matrix data streaming from disk
\item Compare/contrast to bwt-merge

  \end{itemize}



* Incremental Construction and Merging of Succinct Colored de Bruij Graphs}

** Introduction

The motivation for merging succinct colored de Bruijn graphs is that it can mitigate problems with the long time it can take to construct these data structures from scratch.  An example of this construction time is that it took the better part of a week to build the data structures for the 87 metagenomic samples in the VARI paper.  This could pose a problem if we want to apply this data structure to food safety screening projects such as GenomeTrakr.  In such projects, the goal is to isolate a pathogen from an infected patient presenting foodbourne illness and then track down the upstream source in the food supply chain.  The source is found by searching a data structure housing samples from wide swaths of food producers.  If the upstream source can be identified, any additionally tainted product in transit to consumers can be recalled and prevent further illness.  However, in order to identify the problem food source using the patient's isolated pathogen, a similar strain from the problem food source must already be in the database.  Thus, if the succinct de Bruijn graph must be rebuilt from scratch in order to include new samples, then there will be a lag before the newest samples would be findable, which are arguably the most important ones for saving people from infection.  This method would have applications in any project studying a population where more samples are acquired over time.


*** Related Work

*** Contribution

We develop a method for merging succinct colored de Bruijn graphs.  We demonstrate it's efficiency in producing a massive succinct colored de Bruijn graph from a large and small one compared to rebuilding from scratch.  We also introduce a novel efficient algorithm for merging BWT based data.

Allows for incremental update in the form of merging two instances of the succinct colored de bruijn graph into a single instance.
 


** Background
 


** Methods

Now we turn to our method and the rationalle behind it.  We divide the merge process into two stages: 1) planning and 2) actual merging.  We'll give an overview of these and then dive into each in detail.

The backing data structures for VARI are immutable -- we can't simply insert elements from one input into the other, so the backing data structures for the final merged product must be built from scratch. The final data structure can be built by adding new elements to the end, so our approach will be to scan through input data structures in parallel, building the final product up as we go through a series of append operations.  Thus this can be done largely as a streaming operation.

In order to know how to combine input elements, we need first develop a plan.  This is because the elements we are combining, elements of arrays $W$, $L$, and XXX denote edge $k$-mers, and the BOSS machinery needs to be unwound to the fully denoted $k$-mer values and graph topology.  This unwinding requires random access.

*** Planning

The natural place to start is considering that the implicitly represented edge $k$-mers must be sorted.  BWT-MERGE satisfies this by performing backward search in one input of the strings\footnote{The strings are paths in a reverse trie, to save redundant work} in the other input.  We could theoretically use this same approach.

However, there are special properties of the BOSS datastructure we should consider.  Each element in our BWT denotes a $k$-mer, which is a \emph{prefix} of a suffix instead of just a suffix, and these are bounded in length.  This affords the opportunity to merge efficiently using a radix based approach.  Basically, we take the elements of the BWT (which represent the last column of the so called Burrows Wheeler matrix) and map them  their first column  location using the BOSS LF() analog operation\footnote{In a true BWT, this mapping would be a permutation, however symbols may disappear or be duplicated in BOSS based on the number of PREDECESSORS???? of a node}.  Because this needs to be done for all edges, these LF() computations can be done in BWT order, which is more efficient than typical because the in-order access avoids all cache misses and every element is decompressed no more than once.  Given the first column, the partial order and combination of the input elements can be computed.  This first column can then be permuted using the LF() data, again as an in-order traversal of vector BWT, to permute the first column into the second column of the burrows wheeler matrix.  This allows refinement of the partial order previously determined as we examine the symbols from progressively longer suffixes.  This process can continue for $k$ iterations to produce the final order and combination.

Now we look in more detail at how to represent the partial order. Until all columns of the BWT matrix have been generated, there may be multiple rows from the left source BOSS having identical suffixes with those from the right.  The extreme case is the initial case:  based only on their zero length suffixes, all rows from the left are in the same equivalence class to all those on the right.   We capture this by grouping all the left edges in one set and all the right edges in another.   We encode these sets as two bit vectors, one for the left source one for the right source.  A bit vector will contain a unary encoded run of `1's delimited by a final `0'.  Runs of the same rank between the two sets represent the same equivalence class.  When the first (rightmost column) is generated by permuting BWT, we partition the existing set(s) of edges (only one set existing initially) based on the new column -- As we scan through this column, when a differing symbol is encountered we insert a '0' (but only if the point of transition is not already delimited as the end of the run of edges belonging to that set).

*** BOSS Integrity Maintanance
Aside from merging the BWT elements appropriately based on their full implicit edge labels, BOSS includes auxiliary dummy edges and annotation data to enable graph traversal.  Here we discuss how this can be computed during the merge.

The out degree and in degrees of nodes are represented in BOSS by an edge annotatio bit vector and flags on edge labels, respectively.  The bit vector $L$ denotes runs of consecutive edges that all share the same origin node.  The flags denote a series of non consecutive edges that all share the same $K-1$ suffix.  

***** {Origin Node Delimiting}
The bit vector $L$  encodes equivalence classes for nodes based on the $K-1$ edge prefix with a `1' in the first position of a new equivalence class.  These equivalence classes are identical to those computed in the merge plan bitvectors after $K-1$ scans.  Thus these plan bitvectors can be preserved and consulted during the final merge pass.  

***** {Destination Node Delimiting}
All edges sharing the same $K-1$ suffix will be included in a single equivalence class after $K-2$ iterations of the basic planning algorithm.  However, not all $K-1$ suffixes in this equivalence class will be same because the final symbol will vary.  It is still useful for generating flags, as a $\sigma$-length bit vector can maintain whether a given final symbol is the first to be encountered in the $K-2$ equivalence classes.  

***** {Obsolete Dummy Omission}
Because a more complete path may exist in one source graph that overlaps with a tip in the other, the dummy edges differ either in a run of `\$'s at the beginning of their label, or in their final symbol, we may check if a node containing a `\$' shares a suffix with other nodes based on the equivalency class bit vectors and BWT at the point of first discovery of a `\$'

*** Merging
After all columns have been scanned (including the final column,  BWT, which is lexicographically lowest sort priority), the number of equivalence classes will be equal to the number of edge k-mers in the merged output and runs in either vector will be of maximum length 1.  This fully refined structure also serves as the merge plan.  The elements of the two source BWT vectors may then be merged by scanning through the two bit vectors in parallel, one `0' delimited equivalence class at a time, including the next unused BWT symbol from the corresponding source whenever there is a `1'.


The merge follows the plan to combine the BOSS structures.  One complication is that when a new color matrix row is constructured, its reverse complement must also point to that row.  To accomodate this, we maintain a vector of pointers annotating on each source matrix row where its elements were copied in the destination matrix.  Regardless of whether the forward or reverse complement of a canonical k-mer is encountered first during the merge, they will both point to the same source matrix row.  An entry in our annotation vector corresponding to the pointed at row can be used for book keeping to make sure both forward and reverse complement point to the same single row in the newly constructed matrix.
 


