%\section{Methods}
\label{sec-methods}

%In this section, we give our problem formulation, the algorithm behind $\dopp$, and some important implementation details.  The three main insights that allow us to build an index-based aligner for Rmap data is the abstraction of the alignment problem to a finite automaton, the use of the GCSA for storing and queuing the automaton, and the modification of the backward search algorithm to use the wavelet tree.  Each of these insights is explained in detail.

%constructed from the raw Rmaps, which we explain first. We then go on to describe how the path matching problem can be solved efficiently if the appropriate  index data structures are used.

%\subsection{Problem Formulation}
% C: consider using mathematical notation previously introduced, use cut sites, or fragments, but not both

%in both the query and target sequences, just as with ordinary sequence alignment. 
%It is trivial to explore the possibility that a known restriction site in one sequence is missing in the other by substituting a hypothetical compound fragment made from two (or more) expirimentally observed and measured fragments in the input data, however it is infeasible to explore all possible pairs (or triples, etc.) of hypothetical fragments that might exist an a correctly aligning rmap that would sum to a measured fragment in the query.  %break up this sentence.  C asked if this sentence is even necessary, explain why.
%Because of this, hypothetical combined fragments must be constructed in both the query and target.  

%Equivalently, if we consider only the case of missing restriction sites, generating compound fragments is equivalent to removing data from our input, which is the only option before alignment.  
%In other words, given just one sequence, we can determine what other sequences would be concordant if the other sequence was missing data that is present in the one sequence.  % C suggests removing this sentence
% related: given a multiple alignment of rmaps, we can make inferences about which restriction sites in the data are genomic and which are erroneous based on the low frequency of errors a consensus will quickly converge on the genomic sites, however with only two Rmaps as in the pairwise problem we've formulated, we can't say which is a false/spurious site and which is a missing site.  If we can address one of these two possibilities in both the query and target sequences, then all credible alignments can be found.  We consider only the missing site scenario, and understand that if given ground truth or consensus among multiple alignments, what we've deemed a missing site in one is actually a spurious site in the other.
%A reasonable alignment should include a certain amount of common cut sites, though cut sites that do not match are effectively removed from consideration.  In effect, an alignment comprises two rmaps, each representing a possibly different degredation of some third unseen ideal rmap.  If there exists some possible ideal rmap for which both of two rmaps in the dataset are plausible degredations, then we say those two align. % C: do we needs


%%
%%Boucher (October):  One of the applications of the GCSA includes using an automaton to encode a set of observed variations of a pan-genome from the multiple sequence alignment of the genomes of many individuals.  Our analog to this is that we build an automaton encoding all of the hypothetical rmaps that are noisier than an input rmap but that would still align to that rmap. % c: can cut this

%Our solution to this involves combining the use of the GCSA with a wavelet tree to efficiently find all alignment of the query sequence.  

%Hence, $\dopp$ finds all alignments in the following steps: a GCSA index is built for $\R_1 \ldots \R_n$, a wavelet tree is constructed from the GCSA, and a modified version of the FM-index backward search algorithm is used to find the alignments using both data structures.

\subsection{The Rmap Alignment Problem}

The problem of aligning Rmap data involves comparing one Rmap (the \emph{query}) against the set of all other Rmaps in the dataset (the \emph{target}). We will denote the query Rmap as $\R_q$ and the target database as $\R_1 \ldots \R_n$  where each $\R_i$  is a sequence of fragment sizes, i.e, $\R_i = [f_{i1}, .., f_{im_i}]$.  We note that $\R_q$ is contained in $\R_1 \ldots \R_n$.  An alignment between two Rmaps defines a relationship between them that associates groups of zero\footnote{Small fragments may align to nothing in the case of inconsistent desorption.} or more fragments in one sequence with groups of zero or more fragments in the other sequence, such that the sum of fragments between two assocaited groups agree.  More that one fragment in a group occurs when the restriction site delimiting the fragments does not exist in the associated Rmap.  In both the data structure and search algorithm, we address this case and employ the concept of a \emph{compound fragment} for the purpose of discovering alignments. A compound fragment is one or more consecutive fragments from an Rmap whose lengths are summed and the sum treated as though it was a single fragment.  When alignments are found using these compound fragments, their constiuent fragments form the groups of an alignment.

\subsection{Scoring an Rmap Alignment}

%A quality value can be assigned to an alignment based on how well the amount of DNA measured between conserved restriction sites agree, subject to an assumed statistical model of the sizing error, as well as how well the pattern of unmatched restriction sites fits with assumptions about the process that generates the Rmaps.  

We note that for any alignment between any given pair of Rmaps, the Chi-square cumulative distribution function (Chi-square CDF) can be computed.   Alignments are useful because they approximate the unavailable information about common genomic origin of the Rmaps.  In this case, we desire only alignments of sufficient quality that they plausibly do share common genomic origin.  Given a set of Rmaps and input parameters $\rho$ and FOO, we aim to determine all Rmap alignments that have a Chi-square CDF statistic less than $\rho$, and a ratio of matched to missed restriction sites greater than FOO subject to the additional constraint of a maximum consecutive missed restriction site run between aligned sites of $\delta$ and a minimum aligned site set cardinality of 10. 

In fact, a key component of evaluating the quality of the alignment of subsequences of an Rmap are not independent; The stretch of fragments between any two conserved regions in an alignment must have similar total length.  In other words, despite missing or spurious sites, the remaining contextual data flanking the error still has a level of consistency to it (e.g. ``ATCTA'' aligned to ``ATTA'' has a ``C'' deleted and no remnant in the opposing sequence, while [7,5,10,13] aligned to [7,15,13] has the site delimiting 5 and 10 deleted, however the similar sum property is preserved and indicates it's still a reasonable alignment).  This consistency should be used to inform the quality of an alignment.

\subsection{Finite Automaton}

%One significant challenge in aligning one Rmap to another is overcoming missing fragments and missing cut sites. This challenge is analogous to dealing with insertions and deletions in the alignment of long reads---in fact, it is arguably harder since the data is integral and susceptible to inaccuracy.   % FIXME: not sure I understand what principle this statement is getting at
 % This non-independent phenomenon occurs with the fragment size view; that view makes the site position independent; if you take the cut site position view of Rmap data, then missing sites aren't as big a deal, but you have to deal with finding the basis for the two sets of cut-sites
% FIXME: maybe choose number sequences spelling paths in the atomaton in the paragraph above

Continuing with our example, we need to be able to align $\R' = 7,6,3,4$ to $\R = 2,4,5,3,5$ and vice versa.  It will be useful to cast this Rmap alignment problem to matching paths in a finite automaton.   A finite automaton $A = (V, E)$ is built for a set of Rmaps $\R_1..\R_k$ by creating a set of vertices $v^i_1 .. v^i_m$ and edges for each Rmap $\R_i$.  $\A$ contains a {\em starting vertex} $v_1$ labeled with $\#$ and a {\em final vertex} $v_f$ labeled with the character $\$$.  All other vertices in $\A$ are labeled with integral values.    We note that in the most basic form a finite automaton is  a directed, labeled graph that defines a language to be recognized by the sequences that are recognized by the automaton; a sequence is recognized by an automaton if there exists a path {\em matching} that spells the sequence in the automaton.

For each $\R_i$ we need to address all combinations of presence/absence of each restriction site as any collection of them could be missing in other Rmaps which should align. We create $\A$, with each vertex modeling a compound fragment, such that all such combinations are compactly encoded as paths from $v_1$ to $v_s$ in $\A$.  We refer to the {\em backbone} of $\A$ as comprising the sequence of fragment sizes as given in the input data. We refer to the added {\em skip vertices} in $\A$ as those providing alternative subpaths around two or more vertices in the backbone.  These model \emph{proper} compound fragments and allow matching inferred missing restriction sites for other sequences that should align to $R_i$.  First order skip vertices are each labeled with the sum of two consecutive backbone vertices.  Second order skip vertices are each labeled with the sum of three consecutive backbone vertices. %They provide a matching path segment for a sequence with one or more missed restriction sites.  % does this wording make sense? -mm Yes. -sjp

%FIXME: O() notation refers to concatenation of target Rmaps, but we may not be introducing that till later
In our automaton model, vertices represent compound fragments and edges represent restriction sites. Every site in the original target data is potentially conserved in an aligning Rmap, however the preceding and following sites may or may not be.  This means that for every site in the original data, there will be multiple edges in our automaton corresponding to that site: those joining two backbone verticess, as well as all combinations of backbone and skip vertices whose alignment includes that site.  In fact, for every site there will be a complete bipartite subgraph, such that our automaton will have $O(\ell \delta^2)$ edges, where $\ell$ is the concatenation of all Rmaps in the target and $delta$ is the maximum run of consecutive missed restriction sites allowed.

%If we consider all of the possible Rmaps whose spelling path includes a particular restriction site, we'll see that they may contain any of the compound fragments combining target fragments of the site's edges left endpoint; Similarly, they will also contain any single one of the compound fragments of the site's edges right endpoints.  Thus, a site in the target data introduces a junction.  For a sequence to match some path in $A$, there must exist edges between all possible vertices whose right fragment end to those whose left fragment end.  For each restriction site begetting separate fragments $R_i$, there will be a complete bipartite graph comprising all possible combinations of skip and backbone vertices that abut the restriction site.  Essentially, all original and compound fragments whose right ends align to the left side of a particular site will have edges to all those original and compound fragments whose left ends align to the right side of the site.

  Figure \ref{fig:example} illustrates the construction of $\A$ for a single Rmap and only one missing restriction site between those present.  The backbone of this automaton is $\#, 3, 18, 4, 17, 23, 83, 6, \$$ and $22$ connecting $3$ and $17$ is an example of a skip vertex, which provides a matching path for a sequence with a missing fragment due to desorption.  Likewise, $21, 21, 40, 89, 106$ are other skip vertices. Hence, the skip vertices enable us to handle missing cut sites (the first type of error) that occur in the sequence to be matched, or equivalently spurious cut sites in the backbone.  

In addition, we add multiple sets of {\em skip edges} where each set spans a run of small labeled vertices in the backbone.%  The dashed edges in the figure \ref{fig:example} are the skip edges,
They allow us to handle missing fragments (desorption, the second type of error).  
%Since such fragments that may be desorbed in an appropriately aligning Rmap may be flanked by fragments with missing restriction sites, these skip edges form a complete bipartite graph between all skip vertices aligning to the preceding backbone vertex to all skip vertices that succeed the possibly desorbed vertex.  
In theory, a run of such small vertices in the backbone should have skip edges that allow all possible combinations of their presence or absence in query Rmaps.  In order to avoid introducing an excessive number of edges, we assume that any small fragment can suitably match any other small fragment. With this assumption, we introduce edges that skip just the first small fragment, then the first and second, and so-on, all with left endpoints attached to vertices preceding the first small fragment in a run.  These skip edges model the scenario where a fragment present in the target may be desorbed in a query.  In this scenario, every pair of endpoints for skip edges in the backbone must have a full set of additional skip edges between all skip vertices and backbone vertices that align to said backbone endpoints. 
This graph construction allows for any combination of missing and spurious restriction sites as well desorbed fragments.  As we explain below, when the automaton is implemented appropriately, inaccuracies in the fragment sizes (third type of error) can be handled efficiently too. % I added the reason mid paragraph that occured at the end, maybe revert this change? -mm It reads well. - sjp
% are handled by using a {\em wavelet tree}.  The wavelet tree and the space succinct data structure used to implement the finite automaton for the  detection of alignments will be discussed in the next section.



\begin{figure*}[htb]
  \centering
  \includegraphics[width=\textwidth]{../content/figures/example_2}
  \caption{An illustration of a finite automaton that constructed for a small number of of Rmaps.  The backbone vertices are $\#, 3, 18, 4, 17, 23, 83, 6, \$$.  The skip vertices are  $21, 22, 21, 40, 89, 106$ in this example and the skip edges are the dashed edges.  There is a skip edge that allow the omission of fragments of size less than or equal to 6.}
\label{fig:example}
\end{figure*}


%As a result of allowing inexact matches, there may be multiple fragments in an optical map that could each be a reasonable match for an {\em in silico} digested fragment, and in order to include all of these as candidate matches, backtracking becomes necessary in the backward search.
%For every backward search path that maintains a non-empty interval for the entire query contig, we emit the alignments denoted by the final interval.%, mapped back to a position in the optical map.

\subsection{Indexing the Rmap Automaton}

To index our database of Rmaps, we first concatenate the $\R_1 \ldots \R_n$ together into a single sequence with each Rmap separated by  a special symbol which will not match any query symbol. Let $\R^*$ denote this concatenated sequence\footnote{In fact, because the orientation of the Rmaps is unknown, we index Rmaps both forward and backward, such that if one of the ends of a query Rmap aligns somewhere in the middle of some other Rmap, it can be found regardless of their relative orientation in the input data.}. Hence, $\R^* = [f_{11},..,f_{1m_1}, \ldots, f_{n1},..,f_{nm_n}]$.  Next, we construct a finite automaton $\A$ for $\R^*$.  The {\em backbone} of $\A = (V, E)$ is formed by a vertex for each $f_{im_j}$ in $\R^*$ and edges connecting them in input order.  A start vertex and end vertex is added to the beginning and ending of the backbone in $\A$, respectively. Next, given a parameter $s_e$, we add a skip edge between all pairs of vertices in $V$ that are $2,..,s_e$ distance apart.  Lastly, given a parameter $s_v$, we add a skip vertex for any pair of vertices in $V$ that are $2,..,s_v$ distance apart, and join this vertex by edges between the pair.  Additional edges are added between pairs of vertices on opposing sides of a restriction site they abut to complete a bipartite graph modelling all paths including that site.  This GCSA (see the background section) is constructed for this automaton.  All queries are then performed using GCSA queries. It follows from the work of Siren et al.~\cite{dag_method} that the GCSA for the Rmap data can be constructed in $O(XX)$-time and $O(XX)$-memory.   

%An FM-index allows exact string search because it encodes the relationship between each pair of consecutive symbols in a text.  The generalization of the GCSA is that the encoded relationship is between vertices in a directed graph.  Under this generalization, an FM-Index is just the special case of a graph formed where every symbol has a single in edge that originates at its predecessor in the text and a single out edge destined for its successor in the text.  The GCSA still includes the BWT component, and like FM-index backward search, an interval on a BWT can denote a subset of the indexed data.  

%In the FM-index, this interval has an equivalent interpretation in a suffix array and in this interpretation, a prefix of all strings in the interval match the suffix of the query searched up to that point.  In the GCSA, each element in the BWT corresponds to vertex in an automaton.  Each vertex has an annotated string indicated the longest common prefix of all strings reachable from it moving forward.  The interval then encodes all vertices whose prefix is consistent with the portion of the query matched up to that point.  

%After the construction of the GCSA, a wavelet tree $\M$ is created that allows us to map from positions in $\R_q$ to positions in $\R^*$ in constant time.

 
%Additional vertices are added to represent hypothetical fragments that could exist for every boundary between two (or more) fragments formed by successful enzyme digestion.  The hypothetical vertices of two combined fragments provide  alternative path segments (consisting of single vertices) around pairs of vertices in the backbone.

\subsection{Querying the Rmap Automaton}

%We emphasize that for ease of exposition, the description of our modifications to and use of the GCSA in the remainder of this section  avoid specific differences between GCSA-based backward search and FM-index backward search. We thus talk about backward searching in the  GCSA as operations on the $\BWT$ of $\R^*$, etc.. We refer the reader interested in the fine-grained differences between the GCSA and FM-index to~\cite{dag_method}. 

Recall that our aim to find all $\R_i$ in the target database whose alignment to $\R_q$ has a Chi-square statistic less than $\rho$, and a ratio of matched to missed restriction sites greater than FOO. This can be computed using a modified version of GCSA backward search algorithm and data structures applied to the automaton described above.  GCSA's search is an extension of traditional backward searching in an FM-Index.  As outlined in the background material, the FM-index backward search proceeds by finding a succession of intervals in the suffix array that progressively match longer and longer suffixes of the query string, starting from the rightmost symbol of the query.   Each additional symbol in the query string is matched in a process taking two arguments: 1) a suffix array interval, the $\Y$-interval, corresponding to the suffixes in the text, $\ell$, whose prefix matches a suffix of the query string, and 2) an extension symbol $c$.  The process returns a new interval, the $c\Y$-interval, where the prefix of each text suffix corresponding to the new interval is a left extension of the previous query suffix. This process is preserved in $\dopp$. This works because each prefix of an alignment is also an alignment, including the base case of an empty prefix.

% C: this paragraph is too long, make shorter or split in two
As previously mentioned, the GCSA was developed for sequences on a nucleotide alphabet. In adapting it for the large numerical alphabet of optical mapping sequences, we made a number of modifications, especially data structures used during backward search.  In particular, we (1) used a wavelet tree to allow efficient retrieval of substitution candidates, (2) modified the search process to optionally join consecutive query fragments so as to match fragments in $\R^*$ missing the interposing restriction site separating the joined fragments; and 
(3) introduced backtracking, both to try substitution candidates (to address the sizing error of both sequences) as well as combinations of compound fragments (to address missing restriction site errors in the target).

In order to accommodate possible errors in the fragment sizes, we determine a set, $D$, of candidate match fragment sizes that are similar in size to the next fragment of $\R_q$ to be matched in the query. These candidates are drawn from the interval of the $\BWT$ currently active in our backward search using the wavelet tree algorithm of Gagie et al.~\cite{GNPtcs11}.  (Recall that this active interval, when applied to a suffix array, represents the suffixes whose prefixes are the matched portion of the query, while the same interval applied to the $\BWT$ represents possible extension symbols). To accommodate possible restriction sites that are present in the query Rmap but absent in target Rmaps, we generate compound fragments, tring combining single, pairs and triples of consecutive query fragments by adding their sizes and then again query the wavelet tree for substitutions of the compound fragment.  Summing multiple consecutive fragments is complementary to the skip vertices in the target automaton.  It allows accommodation of missed restriction sites in the target, just as the skip vertices in the target accommodate missed restriction sites in the query.  

Similarly, small fragments may be subject to desorption in either the query or the target.  One could simply prune all the fragments below a certain threshold size, however due to the sizing error, one cannot discard based on the true fragment size and one would certainly not achieve consistency (i.e. a small fragment in two maps covering the same genomic region may be above the threshold in one map and below in another, leading to inconsistent discarding).  Therefore, to find all alignments, some mechanism to allow inconsistent inclusion of small fragments must be included.  We use skip edges to allow paths around fragments found in the target set that might be missing from a query Rmap.  Normally, one might need to include skipping over small fragments in not just the target, but in the query as well. This would however result in further branching and slow down the search.  As an alternative, we use separate thresholds for the query and the target such that we can ensure that if a given genomic fragment only is retained in one sequence, it will be retained in the target. 
%FIXME: EXPLAIN WHY.

%SJP: this next paragraph is not super clear.  MDM: It may not be critical; I think there is an interesting insight/way to view this, but it probably should be in discussion. Commenting it out for now.
%% In fact, the search will potentially consider the equivalent of all the paths that exist in the automaton for that same Rmap.  This could be implemented by doing a depth first traversal of $\R_q$'s subgraph in the automaton (locating its backbone and all corresponding skip vertices and edges), however our implementation reconstructs these paths from the query directly. 
%% (This reduces the problem to finding a common path between an automaton for a query and the automaton of the concatenated target Rmaps.) 
%% This traversal only continues as long as there are some paths in the automaton for which the query path comprising the set of compound or original fragments in the query are a statistically plausible match.  Also note that every possible optimal path in a dynamic programming matrix has a corresponding path through the two automata (the actual one indexed by GCSA and the virtual one generated on the fly from $\R_q$).  


 
% As with other FM-index based searches, the target Rmaps are searched in parallel by means of the match interval to the extent they are similar; however, without quantization, the branching factor can be extremely high for the search for substitutions in early fragments from the query.
% SJP: someone left the commment:... [AND SOMETHING LINKING TO PARARLLELIZATION] --- but we don't need to in this context. We're not talking about parallel processing, but rather the multiple alignments that backward search is able to arrive at in a single search process. These are effectively found "in parallel"  
%To reduce the branching factor we quantize the fragment sizes. 
%We thus quantize the data by building a wavelet tree on all Rmaps and then using a wavelet tree traversal.  In particular, we use the algorithm of Gagie et al.~\cite{GNPtcs11}, which takes $O(|D|\log(f/\Delta))$-time to do the wavelet tree quantization, with $|D|$ is the number of candidate match fragment sizes, $f$ is BLAH and FOO is BLAH. IF UNCOMMENTED, ENSURE DELTA USE HERE DOESN'T CLASH WITH OTHER USE IN THE TEXT -mm
%We also use a wavelet tree on the $\BWT$ or $\R^*$ (and the traversal algorithm of Gagie et al.~\cite{GNPtcs11}) in order to explore the neighborhood of candidate matches efficiently, avoiding enumeration of the entire alphabet at each step of the backward search. These candidates are chosen to be within a noise tolerance $t$.  

%[WHAT IS $\R^*$]
%%sjp: it is the concatenation of the Rmaps. It is defined earlier in this section, so this is fine. 
Since there may be multiple match candidates in the $\BWT$ interval of $\R^*$ 
for $\R_q$, we extend the backward search with backtracking so that each candidate size returned to the search algorithm from the wavelet tree is evaluated, i.e., for a given fragment size $f$ in $\R_q$, every possible candidate fragment size, $f'$, that can be found in $\R^*$ in the range $f - t \ldots f + t$ and in the interval $s \ldots e$ (of the $\BWT$ of $\R^*$) for some tolerance $t$ is used as a substitute in the backward search. Each of these candidates is then checked to ensure that the longer partial alignment formed by a left extension would still satisfy alignment criteria.  This requires that the alignment criteria be \emph{monotone}, that is, that the prefixes of a credible long alignment are also credible alignments.  Since the Chi-square CDF statistic is a P-value, using it as alignment criteria satisfies this requirement.   When this criteria is still satisfied after including a candidate symbol, it is used as the extension symbol in the backward search.  This neighborhood is chosen to be within a radius of three standard deviations about the query symbol. 
 
% $\dopp$ additionally combines sequences of fragments in the query to form hypothetical fragments, to allow known fragments in the query to be missing in the target. 
 




\subsection{Practical Considerations}
%\paragraph{Ensuring Overlap Detection}
%One application of aligning Rmap reads is to find pairs that plausibly both cover the same region of the genome.  As the production of the Rmaps is a random process on the genome, their end points will be randomly distributed, however if they do cover the same region of the genome, then one end of one will be covered by the other.  This will hold whether they align in a ``fit'' overlap where one covers genomic regions that extend beyond the ends of the other's coverage on both ends, or ``overlap'' alignments where each covers a region the other does not on opposite flanking regions of the common region.  Thus all overlaps can be found by starting searches at both ends of all Rmaps.  

\paragraph{Alignment Length}
Overlaps must have enough conserved sites to overcome the random chance of excessive spurious alignments introduced by the necessary sizing error tolerance and diversity of ideal fragments. Thus we reject otherwise successful alignments when we reach the end of a query Rmap if the alignment contains too few conserved sites.  

\paragraph{Trimming Tips}
One practical consideration is that the first and last fragment in an Rmap have one edge cleaved by an enzyme and the opposite edge cleaved by a random sheering process that formed the ends of the some chromosomal DNA molecule being mapped.  As such, in general the size of these end fragments will not agree well with a corresponding fragment of an appropriately aligning Rmap, but will be shorter (ignoring errors in cut site presence).  We expect that in practice this expected inequality could be used to slightly refine the informativeness of alignments, but for greater simplicity in our formulation, we trim these end fragments and rely on the interior fragments' agreement to evaluate alignments.

\paragraph{Pruning Queries}

One side effect of summing consecutive fragments in both the search algorithm and the target data structure is that several successive search steps with agreeing fragment sizes will also have agreeing sums of those successive fragments.  In this scenario, proceeding deeper in the search space will result in wasted effort. 
%Since both the automaton encoded in the GCSA and the search algorithm may consider compound fragments by joining consecutive fragments from the input data, there is a risk of spending time searching for and reporting redundant matches where the search eliminates the same restriction sites that actually match each other from both sequences.  
To reduce this risk, we maintain a table of scores obtained when reaching a particular suffix array interval and query cursor pair. We only proceed with the search past this point when either the point has never been reached before, or has only been reached before with inferior scores. We use both a Chi-squared score, and the t-score as computed by Valouev et al. which rewards each matched cut site, and penalizes each missed site.   

%Just as there are multiple paths that originate from a particular cell in a dynamic programming matrix, we may reach the same (SA interval, query cursor) tuple multiple times.  Our search algorithm chooses the most inclusive (best t-score, or equivalently most restriction sites) path first, reducing the chance that paths beyond that point will need to be repeated.  If the path to a particular point with a superior score is found, it can be necessary to repeat the search beyond that point as some ultimately high-quality paths may have been pruned for insufficient score.
%% FIXME: This may be the part reviewer1 complained about being a poor description of dynamic programming.  This is actually a kind of memoization (which is related to DP).  

\paragraph{Quantization}
The alphabet of fragment sizes can be large considering all the measured fragments from multiple copies of the genome.  This can cause an extremely large branching factor for the initial symbol and first few extensions in the search.  To improve the efficiency of the search, the fragment sizes are initially quantized, thus reducing the size of the effective alphabet and the number of substitution candidates under consideration at each point in the search.  This does introduce some quantization error into the fragment sizes, but the bin size is chosen to keep this small in comparison to the sizing error.



%% \subsection{Implementation}

%% The noise tolerance parameter $t$ has a default of BLAH, which is chosen based on the assumptions about the distribution of optical measurement error around the fragment size.  The default standard deviation is calculated using the .58 kb/1 kb value encoded in the software from Valuev et al..   We use a version of this algorithm implemented in the SDSL-Lite library for BLAH. 
