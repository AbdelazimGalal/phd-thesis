%======================================================================
\chapter{Smoothed Anaylsis of {\sc Closest String} Problem}\label{chapter:smoothed_analysis}
%======================================================================

In the previous chapter, we described how the {\sc Closest String} problem can be solved efficiently when we restrict interest to either instances where the number of strings is significantly large and where the number of strings is significantly small; for both cases we develop efficient algorithms.  The analysis for the algorithm of the latter case brought about the introduction of smoothed analysis to the {\sc Closest String} problem. We continue this area of research in this chapter.  More specifically, we consider the smoothed analysis of an existing algorithm for {\sc Closest String} problem and show that more complex analysis of it's running time elucidates it's practicality.  

One approach to investigating the computational intractability of the {\sc Closest String} problem is to consider its parameterized complexity, which aims to classify computationally hard problems according to their inherent difficulty with respect to a subset of the input parameters. As discussed in Chapter \ref{chapter:related_work}, Gramm {\em et al.} \cite{GNR03} proved that {\sc Closest String} is fixed parameter tractable with respect to the parameter $d$ by giving a $O(n\ell + nd \cdot d^d)$ time algorithm that is based on the bounded search tree paradigm that is frequently used in parameterized complexity.  This algorithm will be of particular importance in this section. 

In 2008, Ma and Sun gave an $O(n |\Sigma|^{O(d)})$ algorithm, which is fixed parameter tractable with respect to $d$ and $\Gamma$ \cite{MS08}. Hence, if $d < |\Sigma|$ the fixed parameter tractability algorithm of Gramm {\em et al.} \cite{GNR03} is the most efficient and otherwise the algorithm of Ma and Sun is better \cite{MS08}.  It is open as to the existence of an algorithm that is fixed parameter tractable with respect to $d$ that is exponential in a constant independent of $d$ or $\Sigma$.  

In addition to worst-case theoretical analysis, Gramm {\em et al.} \cite{GNR03} gave experimental results that suggest the $O(n\ell + nd \cdot d^d)$ algorithm is far more efficient in practise.  The worst-case analysis is relies on showing that the size of the binary search tree is at most $(d + 1)^d$; whereas, the experiment results demonstrate that the search trees are by far smaller than this bound predicts ({\em i.e.} in the range of size $d$). Although, the empirical results are compelling and suggest the practicality of this algorithm, there currently does not exist an analytical reason for the efficiency of the algorithm.

We initiate the study of the smoothed complexity of the algorithm by Gramm {\em et al.} \cite{GNR03}, which in turn explains the efficiency of this algorithm and specifically, the empirical  Gramm {\em et al.} \cite{GNR03}. The concept of smoothed analysis was introduced as an intermediate measure between average case analysis and worst case analysis; whereas average analysis studies the average behaviour of an algorithm over all instances of a problem, smoothed analysis studies the algorithm's average behaviour on each ``local region'' of the instance space \cite{ST01}.  If the algorithm has good average performance on each local region, then for any reasonable probabilistic distribution on the whole instance space, the algorithm should perform well.  If the smoothed complexity is low, worst case instances are not robust under small changes. Most small changes to the instance destroy the property of being worst-case; a small random perturbation destroys the property of being worst-case. 

We consider a slight modification of this algorithm by Gramm {\em et al.} \cite{GNR03}.  Rather than beginning with a random string (as in the original algorithm) we will assume that the algorithm begins with a {\em majority vote string}, which is a string that contains the majority symbol at each position with ties broken arbitrarily.  Our main contribution demonstrates that for sufficiently large values of $n$ and $\ell$, and perturbation probability $p$, the modified algorithm builds a search tree with expected size $d^{2 + \epsilon}$, where $\epsilon$ is a small value greater than zero. In Section \ref{smoothed_analysis_section} we introduce a reasonable perturbation model for the {\sc Closest String} instances and prove that the average running time of the algorithm of Gramm {\em et al.} \cite{GNR03} is $O(n\ell + nd \cdot d^{2 + \epsilon})$ for any given perturbed instance.  This analysis involves investigating typical properties of a smoothed instance, including providing a bound for the probability that any sequence that contains the majority alphabet symbol at each each position is a center string.  From a practical perspective, these results explain why this fixed parameter algorithm for the {\sc Closest String} problem performs well in practise. 

The results in this section show the practicality of the simple and well-known fixed parameter tractability algorithm of Gramm {\em et al.} \cite{GNR03} by showing the expected size of the search tree on smoothed instances $d^{2 + o(1)}$. Hence, resolving the open problems suggested by Gramm {\em et al.} \cite{GNR03}, and Ma and Sun \cite{MS08}.

\section{Preliminaries}

Let $s$ be a string over the alphabet $\Sigma$. $|s|$ denotes the length of $s$ and $s[j]$ denotes the $j$th letter of $s$.  Hence, $s = s[1]s[2]\ldots s[|s|]$.  We refer to a {\em majority vote} for $S$ as the $\ell$-length string containing the letter that occurs most often at each position; this string is not necessarily unique. The following fact, which is easily proved, is used in Section \ref{smoothed_analysis_section}.  

\begin{fact} \label{fact2} Let $I = (S, d)$ be a {\sc Closest String} instance and $s_{maj}$ be any majority string for $S$ then $d(s^*, s_{maj}) \leq 2d$ for any a center string $s^*$ for $S$.  \end{fact}

\begin{proof} We assume $n$ is even. Suppose otherwise that there exists an instance $I = (S, d)$ where one of the majority strings $s_{maj}$ has distance $2d + 1$ from the center string $s^*$.  Without loss of generality we assume $0, 1 \in \Sigma$, $0^{\ell}$ is the center string, and $1^{2d + 1}0^{\ell - 2d - 1}$ is $s_{maj}$.  Thus, there exists at least $n/2$ strings equal to $1$ at the first $2d$ positions, implying each string in $S$ has $d$ positions equal to $1$.   Since each string has distance at most $d$ from $0^{\ell}$ we contradict $d(s^*, s_{maj}) > 2d$.   An identical proof can be given for the case when $n$ is odd.  \hfill $\Box$ \end{proof} 

We will use the following asymptotic estimation, which is proved in the previous section.

\noindent{\bf Fact \ref{fact1}} For $i > 0$ and $j > 0$ the following asymptotic estimation exists: $$\sum_{i = 0}^{j} {{2i + j}\choose{i}} \left( \frac{\ell - 1}{\ell}\right)^{i} \left( \frac{1}{\ell}\right)^{i + j} \asymp \left( \frac{1}{ \ell - 1}\right)^j.$$ 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Bounded Search Tree Algorithm} \label{sec:search_tree_alg}

Algorithm \ref{alg:CSA} outlines the algorithm of Gramm {\em et al.} \cite{GNR03} that applied a well-known bounded search tree paradigm, and shows the {\sc Closest String} problem can be solved in linear time for constant $d$.  The original algorithm of Gramm {\em et al.} \cite{GNR03}  begins by initializing a parameter $\Delta d$ to $d$ and candidate string $s$ to $s_1$.  Every recursive call decreases $\Delta d$ by one.  The algorithm halts when $\Delta d < 0$. Therefore, the algorithm builds a search tree of height at most $d$.  At each iteration of the recursion, a string $s_i$ such that $d(s, s_i) > d$ and therefore, creates a subcase for $d + 1$ of the positions in which $s$ and $s_i$ disagree (there are at most $2d$ positions and at least $d + 1$ positions).  This yields an upper bound of $(d + 1)^d$ on the search tree size.  We consider a slight modification of this algorithm, where we begin by initializing the candidate string to be a majority vote string.

\begin{algorithm}[h]
\caption{Closest String Algorithm}
\begin{algorithmic}
\STATE {\bf Input: } A {\sc Closest String} instance $I$, a candidate string $s$, and a parameter $\Delta d$.
\STATE {\bf Output:} A center string $s$ if it exists, and ``not found'' otherwise.
\STATE If $\Delta d < 0$, then return ``Not found''
\STATE Choose $i \in \{1, \ldots, n\}$ such that $d(s, s_i) > d$. 
\STATE \hspace{5mm} $\mathcal{P} = \{p | s[p] \ne s_i[p]\}$;
\STATE \hspace{5mm} Choose any $\mathcal{P}'$ from $\mathcal{P}$ with $|\mathcal{P}'| = d + 1$.
\STATE \hspace{5mm} For each position $p \in \mathcal{P}'$
\STATE \hspace{10mm} Let $s(p) = s_i(p)$ 
\STATE \hspace{10mm} $s_{ret}$ = {\bf Closest String Algorithm ($s$, $\Delta d - 1$)} 
\STATE \hspace{10mm} If $s_{ret} \, \ne$ ``not found '', then return $s_{ret}$
\STATE Return ``not found''
\end{algorithmic}
\label{alg:CSA}
\end{algorithm}

For any ``yes'' instance of the {\sc Closest String} problem the majority vote string $s$ has distance at most $2d$ from each string in $S$. It follows that the proof of the running time and correctness of the original algorithm continues to hold for our modified algorithm and therefore, the following theorem holds for our modification.

\begin{theorem} \label{worst_case_theorem} \cite{GNR03} Algorithm \ref{alg:CSA} solves the {\sc Closest String} problem in time $O(n\ell + nd \cdot d^d)$. \end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Smoothed Analysis of Bounded Search Tree Algorithm} \label{smoothed_analysis_section}

We show has running time $O(n\ell + n d \cdot d^3)$, which substantially improves on the worst case analysis of \cite{GNR03}. Our proof relies on considering the smoothed height of the binary search tree that is obtained by randomly perturbing a given (adversarial) instance, then taking the expected height of the search tree generated by this instance.  

%%%%%%%%%%
\subsection{Pertubation Model for {\sc Closest String} Problem} \label{perturb_model}

We begin our analysis by defining perturbation model for the {\sc Closet String} problem.  Since we are deal with string problems we need a perturbation model that slightly changes the instance by randomly choosing a new symbol for a small number of positions.  Further, we need a perturbation model that will conserve the `yes' instances; that is if $I = (S, d)$ is a {\sc Closest String} instance that contains at least one center string, then after perturbing $S$ and $d$ the resulting instance must also have at least one center string.  We now present our model that achieves this constraint.

Let $I = (S, d)$ be a {\sc Closest String} instance that has at least one center string and without loss of generality, assume $0 \in \Sigma$ and $0^{\ell}$ is a center string for $S$. A {\em perturbed instance}  of $I$ is defined to be  $I' = (S', d')$, where $d' = d  + p(\ell - 2d)$ and each string $s_i'$ is obtained by mutating $s_i$ with a small probability $p >0$ as follows: 

\begin{itemize}
\item For each $j$ such that $s_i(j) \ne 0$, we let $s_i'(j) = 0$ with probability $p > 0$. Let $\rho_i = \{j | s_i(j) \ne 0 \mbox{ and } s_i'(j) = 0\}$.
\item Select $\eta_i \in (0, d' - (d(s_i, 0^{\ell}) - \rho_i))$ and choose $\eta_i$ positions uniform-randomly from the set of positions where $s_i$ is equal to 0. For each letter $j \in \eta_i$, we select a symbol in $\Sigma / \{0\}$ to be equal to $s_i'(j)$.
 \end{itemize}

\noindent We note that if $0^{\ell}$ is a center string for $S$ with respect to the parameter $d$ then it is a center string for $S'$ with respect to the parameter $d'$.  For the remainder of this section, unless otherwise stated, we assume that if an instance $I = (S, d)$ contains a center string then $0 \in \Sigma$ and $0^{\ell}$ is a center string.  

%%%%%%%%%
\subsection{Good Columns and Simple Instances}

We define a perturbed instance $I' = (S', d')$ as {\em simple} if all strings in $S'$ have Hamming distance at most $d'$ from each of the majority strings for $S'$.  This definition is used in our final smoothed analysis and is crucial for obtaining a tight bound on the number of instances in which the algorithm performs poorly. Instances that are simple have the property that the algorithm immediately halts with a correct solution and therefore, we are assured that the running time of the algorithm is efficient.  We refer to an column as {\em good} if it contains more zeros than nonzeros and thus, guarantees that the majority symbol is equal to the center string at that position; all other columns are {\em bad}.   

\begin{lemma} \label{lem:simple} Let $I' = (S', d')$ be a perturbed instance with probability $0 < p \leq \frac{1}{2}$ then the probability that $I'$ is not simple is at most $1 - \left( 1 - \frac{1}{2^n} \right)^{\ell}$.\end{lemma}

\begin{proof} We first calculate the probability that a column is good.  Let $X_{i, j}$ be a binary random variable that is equal to 1 if $s_i$ is equal to the value of the center string ({\em i.e.} equal to $0$) at the $j$-th position.  For a given $j$, let the number of ones be $X_j = \sum_{i} X_{i,j}$.  

\begin{eqnarray*}
\Pr[X_j > n/2] 	& = & \sum_{i = n/2}^n {n \choose i} \left(\Pr[s_i'(j) = 0]\right)^i \left(1 - \Pr[s_i'(j) = 0] \right)^{n - i} \\ 
						& = &(1 - \Pr[s_i'(j) = 1])^n  \sum_{i = 0}^n {n \choose i} \left(\frac{\Pr[s_i'(j) = 1]}{1 - \Pr[s_i'(j) = 1]} \right)^i  \\
						&~&~ - (1 - \Pr[s_i'(j) = 1])^n \sum_{i = 0}^{n/2 - 1} {n \choose i} \left(\frac{\Pr[s_i'(j) = 1]}{1 - \Pr[s_i'(j) = 1]} \right)^i 
\end{eqnarray*} 

It follows from this last equation that: 

\begin{eqnarray}\label{prob_good_column}
1  - \Pr[s_i'(j) \ne 0]^n	\leq  \Pr[X_j > n/2] 	 \leq 1  - \Pr[s_i'(j) \ne 0]^{n/2 + 1}. 
\end{eqnarray} 

%We can show the following lemma using a similar proof to the one above. \begin{lemma} \label{lem:good_column2} Let $I' = (S', d')$ be a perturbed instance with probability $p$ then the probability a column of $S'$ is bad is at least $1  - \Pr[s_i'(j) = 0]^n$ and at most $1  - \Pr[s_i'(j) = 0]^{n/2 + 1}$. \end{lemma}

To determine the probability that $I'$ is not simple we calculate the probability $0^{\ell}$ is a majority string and hence, that $I'$ contains at least one bad column.   

\begin{eqnarray*}
\Pr[I' \mbox{ is not simple}] 	& = & 1 - \Pr [X_j > n/2 \, \, |  \, \, j \in [0, \ell] ] \\ 
												& \leq & 1 -  \left( 1 - \Pr[s_i'(j) \ne 0]^n \right)^{\ell} \, \, \mbox{ By Equation \ref{prob_good_column}} \\
												& \leq & 1 - \left( 1  - \left(\frac{d'}{\ell}\right)^n\right)^{\ell} \\ 
												& \leq & 1 - \left( 1  - \frac{1}{2^n}\right)^{\ell}
\end{eqnarray*} \hfill  $\Box$ \end{proof}

%%%%%%%%%%%%
\subsection{Smoothed Height of the Search Tree}

In this subsection we prove one of the main results of this work, namely an upper bound for the expected height of the bounded search tree produced by Algorithm \ref{alg:CSA} under the perturbation model defined in Subsection \ref{perturb_model}.  Before giving this result we prove an important property for estimating the smoothed height of the search. More specifically, we give an upper bound for the probability that the size of the search tree exceeds a specific value.

There are $O(d^d)$ possible paths in the search tree $\mathcal{T}$ corresponding to Algorithm \ref{alg:CSA}.  Let $P_i$ be the random indicator variable describing whether the $i$th path in $\mathcal{T}$ is does/does not result in a center string; {\em i.e. } let $P_i = 1$ if the $i$th path results in a center string and $P_i = 0$ otherwise.  The algorithm halts when $P_i = 1$. Let $\mathcal{P}$ be the number of paths considered until $P_i = 1$.  

\begin{lemma} \label{lem:bound} Let $\epsilon >0$ be a small number and $0 < p \leq \frac{1}{2}$.  Let $I' = (S', d')$ be a {\sc Closest String} instance that is not simple, then when $\ell$ is significantly large $$\Pr[ \mathcal{P} \geq d^{dpc}]\leq \frac{1}{d^{dpc}}, \, \, \mbox{for any constant $c > 0$}.$$ \end{lemma}

\begin{proof} If $I' = (S', d')$ is a ``no'' {\sc Closest String} instance, the result of each recursive iteration of Algorithm \ref{alg:CSA} will always return false. However, if $S'$ contains a center string, with some probability $P_i = 1$.  Then it is clear that the expected number of paths in $\mathcal{T}$ that need to be considered before encountering  path that leads to a center string is $1/\Pr[P_i = 1]$.  We now calculate $\Pr[P_i = 1]$.  Suppose $I' = (S', d')$ contains a center string and without loss of generality, we assume $0 \in \Sigma$ and $0^{\ell}$ is a center string. 

If the candidate string $s$ in Algorithm \ref{alg:CSA} is not equal to $0^{\ell}$ then there is at least one letter of $s$ that can be changed so that $d(s, 0^{\ell})$ decreases by one; the probability of this occurring is at least $1/\ell$.  Denote $Y_k \in \{0, 1, \ldots, d'\}$ ($k = 0, 1, \ldots$) as the random variable that is equal to the Hamming distance between $s$ and $0^{\ell}$, where $k$ is the number of recursive iterations.  Each time a position is selected and the value of that position is augmented, either the Hamming distance is increased or decreased by one.

The process $Y_0, Y_1, Y_2, \ldots$ is a Markov chain with a barrier at state $d'$ and contains varying time and state dependent transfer probabilities. This process is overly complicated and we instead choose to analyze the following process: $Z_0, Z_1, Z_2, \ldots$,  where $Z_k$ is the random variable which is equal to the state number after $k$ recursive steps and there exists infinitely many states.  Initially, this Markov chain is started like the stochastic process above ({\em i.e.} $Z_0 = Y_0$).  As long as the inner loop is iterating, we let $Z_{k+1} = Y_k - 1$  if the process decreases the Hamming distance between $s$ and $0^{\ell}$ by one; and $Z_{k+1} = Y_k + 1$ otherwise.  After the algorithm halts, we continue with the same transfer probabilities. By induction on $k$, it is clear that for each $k$, $Y_k \leq Z_k$, and it follows that $\Pr[P_i = 1]$ is at least $\Pr[\exists t \leq d: Z_t = 0]$.    

We made the assumption that $S'$ contains only one center string, however, this assumption is not needed -- the random walk may find another center string while not in the terminating state but this possibility only increases the probability the algorithm terminates. 

Given that the Markov chain starts in some state $k$, it can reach a halting state in at least $k$ steps by making transitions through the states $k - 1$, $k - 2$, $\ldots$, $1$, $0$.  The probability of this happening is $(1/\ell)^k$.  Also, for $j = 1, 2, 3, \ldots$ the state can be reached $2j + k$ steps where there are $j$ steps which increase the state number and $k + j$  steps which decrease the state number.  Let $q(j,k)$ be the probability that $Z_{2j + k} = 0$, such that the state 0 is not reached  in any earlier step--under the condition that the Markov chain started in state $k$.  More formally,  $$q(j, k) \, =  \, \Pr[Y_{2j+k} = 0, \, \mbox{ and } \, Z_{\alpha} > 0 \, \forall \, \alpha < 2j + k \, | \, Z_0 = k ].$$ Clearly, $q(0, k) = (1/\ell)^k$,  In the general case, $q(j, k)$ is $((\ell - 1)/\ell)^j (1/\ell)^{j + k}$ times the number of ways of arranging $j$ bad steps and $j + k$ good steps such that the sequence starts in state $k$, ends in state 0 and does not reach 0 before the last step. Using the ballot theorem \cite{ballot_thm} for we know there are ${{2j + k}\choose{j}}\frac{j}{2j +k}$ possible arrangements of these $j$ and $j + k$ steps. Therefore, the above probability is at least $${{2j + k}\choose{j}}\frac{j}{2j +k} \left( \frac{\ell - 1}{\ell}\right)^{j} \frac{1}{\ell^{j + k}}.$$ This expression is not defined in the case $j = k = 0$.  In this case, the probability is equal to 1.  Thus, we have: 

We note that Fact \ref{fact2} states that $2d'$ is maximum distance from any majority string to a center string for any {\sc Closest String} instance.   Next, to calculate $\Pr[P_i = 1]$ we need to calculate the probability the probability that $k$ bad columns exist, for $k = 1, \ldots, 2d$. We let $X_{bad}$ be the total number of columns that are bad in $S'$. It follows from Equation \ref{prob_good_column} and the linearity of expectation, that the expected number of bad columns is $\ell(1 - \Pr[s_i' =0]^n)$.  Therefore we get the following:
  
\begin{eqnarray*}
\Pr[P_i & = & 1] 		 \geq  \sum_{j = 1}^{2d} \Pr[X_{bad} = j]\sum_{i = 0}^j q(i,j) \\
						& \geq &  \left( \Pr[X_{bad} < 2d +1] - Pr[X_{bad} = 0] \right) \sum_{j = 1}^{2d} \sum_{i = 0}^j q(i,j) \\
					 	& = & \left( 1 - \Pr[X_{bad} \geq \ell - 2d - 1] - Pr[I' \mbox{  is simple}] \right) \sum_{j = 1}^{2d} \sum_{i = 0}^j q(i,j) \\
\end{eqnarray*}

\noindent It follows from Markov's inequality that: $$\Pr[P_i  = 1] \geq  \left( 1 -\frac{\ell (1 - \Pr[s_i'(j) = 0]^n)}{\ell - 2d - 1} -  Pr[I' \mbox{  is simple}] \right) \, \sum_{j = 1}^{2d} \sum_{i = 0}^j q(i,j).$$  By Lemma \ref{lem:simple}, we get:

\begin{eqnarray*}
\Pr[P_i  = 1]	& \geq &  \left( 1 -\frac{\ell (1 - \Pr[s_i'(j) = 0]^n)}{\ell - 2d - 1} - \left( 1 - \frac{1}{2^n}\right)^{\ell} \right) \, \sum_{j = 1}^{2d} \sum_{i = 0}^j q(i,j)  \\
					 	& \geq & \left( 1 -\frac{\ell}{\ell - 2d - 1} - \left( 1 - \frac{1}{2^n}\right)^{\ell} \right)\, \sum_{i = 0}^{2d} q(i,j) \\
					 	& \geq & \left( 1 -\frac{\ell}{\ell - 2d - 1} - \left( 1 - \frac{1}{2^n}\right)^{\ell} \right) \, \sum_{i = 1}^{2d - 1} \left( \frac{1}{1 - \ell} \right)^{i + 1}  \, \, \mbox{[Fact \ref{fact1}]} \\
					 	& \geq & \left( 1 -\frac{\ell}{\ell - 2d - 1} - \left( 1 - \frac{1}{2^n}\right)^{\ell} \right) \, \frac{1 - \left(\frac{1}{\ell -1 } \right)^{2d + 1}}{1 - \frac{1}{\ell - 1}}
\end{eqnarray*}

\noindent Hence, for sufficiently large $\ell$ we have $\Pr[P_i = 1] = 1 - \left( 1 - \frac{1}{2^n}\right)^{\ell}$ and it follows that: $$E[\mathcal{P}] = \frac{1}{1 - \left( 1 - \frac{1}{2^n}\right)^{\ell}}$$ and by Markov inequality we have that for any $c > 0$ $$\Pr[\mathcal{P} \geq d^{dcp}] \leq \frac{1}{d^{dcp} \left(1 - \left( 1 - \frac{1}{2^n}\right)^{\ell}\right)}.$$  Hence, $\Pr[\mathcal{P} \geq d^{dcp}]$ is equal to $ \frac{1}{d^{dcp}}$ for significantly large $n$. \hfill $\Box$ \end{proof}

The following is our main theorem which provides an upper bound on $E[\mathcal{P}]$, the expected size of the search tree.  An important aspect about this result is the small perturbation probability require in comparison to the instance size.  For a string of length $\ell$, the expected number of positions to change is approximately $O(\log n)$. 

\begin{theorem}  For any small $0 < \epsilon \leq d - 2 - 3 \log d$ and perturbation probability $$\frac{\epsilon}{\alpha} \left( \frac{\log d}{d} \right)^3 \leq p \leq \frac{\epsilon}{\alpha} \left( \frac{\log d}{d} \right)^2,$$ where $\alpha \geq \frac{d - 2 - \epsilon}{\epsilon \cdot 3 \log d}$, the expected running time of Algorithm \ref{alg:CSA} on the perturbed instances is $O(n \ell + n d \cdot d^{2 + \epsilon})$ for sufficiently large $\ell$ and $n$. \end{theorem}

\begin{proof} There are $O(d^d)$ possible paths in the search tree corresponding to Algorithm \ref{alg:CSA} \cite{GNR03}. The size of the bounded search tree is equal to zero for simple instances and   therefore, we are only required to consider the non-simple instances in which Lemma \ref{lem:bound} holds and ones in which it does not.  For the former instances we will use Lemma \ref{lem:bound} with $c = \alpha \cdot d$ to bound the expected size of the search tree and for the latter instances we use Lemma \ref{lem:simple}.  Lemma \ref{lem:simple} states that the probability that an instances is not simple is at most  $1 - \left( 1  - \frac{1}{2^n}\right)^{\ell}$.  Hence, we have the following bound

\begin{eqnarray*}
E[\mathcal{P}] & \leq & d^{\alpha \cdot d^2p} \cdot \Pr[I' \mbox{ is not simple}] + d^d \cdot\Pr[\mathcal{P} \geq d^{dcp}] \\
 								& \leq & d^{\alpha \cdot d^2p} \cdot \left( 1 - \left( 1  - \frac{1}{2^n}\right)^{\ell} \right) +d^d \cdot\Pr[\mathcal{P} \geq d^{dcp}]  \, \, \mbox{ by Lemma \ref{lem:simple}}\\
 											& \leq & d^{\alpha \cdot d^2p} \cdot \left( 1 - \left( 1  - \frac{1}{2^n}\right)^{\ell} \right) + d^{d - dcp}   \, \, \mbox{ by Lemma \ref{lem:bound}}\\
 											& \leq & \frac{d^{\alpha \cdot 2 \log d}} {2^{n\ell}} + d^{d - dcp} \\
\end{eqnarray*}  

We consider the first term of the last equation.  We have $d^{ \alpha \cdot 2 \log d} \leq 2^{n \ell}$ if and only if $\frac{4}{3 \log 2} \leq 5 \leq \frac{\epsilon \cdot n \ell}{d - 2 - \epsilon}$.  Therefore, for sufficiently large $\ell$ and $n$ we have: $$ E[\mathcal{P}] \leq  o(1) +  d^{d(1 -\frac{\alpha p}{d})}.$$ Next, we show the exponent of the second term is at most $2 + \epsilon$ as follows:

\begin{eqnarray*}
1 - \frac{\alpha p}{d}  & \leq & \frac{2 + \epsilon}{d} \\
d - \epsilon \cdot 3 \log d \cdot \alpha & \leq & 2 + \epsilon \\
d - \epsilon \cdot 3 \log d \cdot \frac{d - 2 - \epsilon}{\epsilon \cdot 3 \log d} & \leq & 2 + \epsilon 
\end{eqnarray*}  

Therefore, we have $E[\mbox{search tree size}] \leq o(1) +  d^{2 + \epsilon}$. The analysis of Gramm {\em et al.} \cite{GNR03} demonstrated that each recursive step takes time $O(nd)$ and the preprocessing time takes $O(n\ell)$ and therefore, we obtain an overall running time of $O(n \ell + n d \cdot d^{2 + \epsilon})$. \hfill $\Box$ \end{proof}

We note that we require $\epsilon$ to be at most $d - 2 - 3 \log d$ in order to have $p \in [0, 1]$ and $\frac{\epsilon}{\alpha} \cdot \left( \frac{\log d}{d} \right)^3 \leq \frac{\epsilon}{\alpha} \left( \frac{\log d}{d} \right)^2$. Also, as shown in the previous chapter and by Gramm {\em et al.} \cite{GNR01}, there exists efficient solutions for the {\sc Closest String} problem when either $n$, $\ell$ or $d$ are reasonably small and therefore, the constraint that $n$ and $\ell$ are sufficiently large is a reasonable assumption. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary}

We have analyzed the smoothed complexity of the {\sc Closest String} and {\sc Farthest String} problems. Smoothed complexity describes the distribution of these hard string selection problem instances and the behaviour of the $O(n\ell + nd \cdot d^d)$ algorithm of Gramm {\em et al.} \cite{GNR03}.  From a practical perspective, this analysis gives an analytical reason to why this--and perhaps other similar {\sc Closest String} algorithms-perform well in practise.  

